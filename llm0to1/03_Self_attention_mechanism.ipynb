{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhcTlXVWxYRzL+hpN6tzil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanjaiswalofficial/machine-learning-engineer-projects/blob/main/llm0to1/03_Self_attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So far\n",
        "Embeddings capture meaning, but not relationships between words. <br/>\n",
        "Positional encodings capture order, but not dependencies between distant words. <br/>\n",
        "Self-attention lets the model focus on important words while processing a sentence.\n",
        "\n",
        "\"The cat sat on the mat because it was warm.\"\n",
        "\n",
        "\"It\" refers to \"the mat,\" not \"the cat.\"\n",
        "Self-attention allows the model to learn these relationships."
      ],
      "metadata": {
        "id": "JM9HWEjfKBIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "V9dHjbeDJ5Nx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim  # Store the embedding dimension\n",
        "    self.scaling = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))  # Scaling factor for attention scores\n",
        "\n",
        "    self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Linear layer for query\n",
        "    self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Linear layer for key\n",
        "    self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Linear layer for value\n",
        "\n",
        "  def forward(self, x):\n",
        "    Q = self.W_q(x)  # Compute query matrix\n",
        "    K = self.W_k(x)  # Compute key matrix\n",
        "    V = self.W_v(x)  # Compute value matrix\n",
        "\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling  # Compute scaled dot-product attention scores\n",
        "\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)  # Apply softmax to get attention weights\n",
        "\n",
        "    output = torch.matmul(attention_weights, V)  # Compute the weighted sum of values\n",
        "\n",
        "    return output, attention_weights  # Return the output and attention weights"
      ],
      "metadata": {
        "id": "FjVRKolMJ8cw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding dimension and sequence length\n",
        "embed_dim = 8\n",
        "seq_len = 5\n",
        "\n",
        "# Create a random input tensor with shape (1, seq_len, embed_dim)\n",
        "x = torch.rand(1, seq_len, embed_dim)\n",
        "\n",
        "# Instantiate the SelfAttention class\n",
        "self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "# Perform a forward pass through the self-attention mechanism\n",
        "output, attention_weights = self_attention(x)\n",
        "\n",
        "# Print the shapes of the output and attention weights\n",
        "print(\"Attention Output Shape:\", output.shape)  # (1, 5, 8)\n",
        "print(\"Attention Weights Shape:\", attention_weights.shape)  # (1, 5, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73MQrk0HKXYy",
        "outputId": "9613fad1-c078-40a8-d364-cda4e04751bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output Shape: torch.Size([1, 5, 8])\n",
            "Attention Weights Shape: torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "Letâ€™s say we have 5 words in a sentence, each represented as an 8-dimensional vector (just a list of numbers)."
      ],
      "metadata": {
        "id": "LC8_kVuaN169"
      }
    }
  ]
}