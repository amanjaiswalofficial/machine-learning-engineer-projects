{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanjaiswalofficial/machine-learning-engineer-projects/blob/main/llm_engineering_works/15_side_mission_training_a_simple_neural_network_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "1. Library Imports & Setup\n",
        "\n",
        "    Hugging Face Libraries: For loading datasets, models, tokenizers, and fine-tuning utilities.\n",
        "    Weights & Biases (wandb): Used for experiment tracking and logging model training metrics.\n",
        "    BitsAndBytes (bnb): Enables quantization (4-bit or 8-bit) to reduce memory usage during training.\n",
        "\n",
        "2. Constants\n",
        "\n",
        "    Model Selection: Base model (meta-llama/Meta-Llama-3.1-8B) for fine-tuning.\n",
        "    Dataset: A custom Hugging Face dataset (pricer-data).\n",
        "    LoRA Configuration: Low-Rank Adaptation parameters (r, alpha, dropout) for efficient fine-tuning.\n",
        "    Quantization: Specifies whether to use 4-bit quantization for efficient memory usage.\n",
        "    Training Hyperparameters: Includes learning rate, batch size, epochs, gradient accumulation, and optimizer type.\n",
        "\n",
        "3. Authentication & Logging\n",
        "\n",
        "    Hugging Face Hub Login: Logs into Hugging Face to push the fine-tuned model.\n",
        "    W&B Login: Sets up the environment to log metrics during training.\n",
        "\n",
        "4. Quantization with BitsAndBytes\n",
        "\n",
        "    4-bit Quantization: Reduces memory footprint and enables training on large models with smaller GPUs.\n",
        "\n",
        "5. Tokenizer Initialization\n",
        "\n",
        "    Prepares the tokenizer to handle input text, with proper padding and special token configuration.\n",
        "\n",
        "6. Base Model Initialization\n",
        "\n",
        "    Loads the pre-trained LLaMA model with quantization configurations (bnb_4bit or bnb_8bit).\n",
        "\n",
        "7. Memory Footprint Check\n",
        "\n",
        "    Estimates the model's memory usage to confirm compatibility with the system.\n",
        "\n",
        "8. Data Collator\n",
        "\n",
        "    DataCollatorForCompletionOnlyLM: Structures input text into training examples for causal language modeling, based on a template response.\n",
        "\n",
        "9. LoRA Configuration\n",
        "\n",
        "    Defines parameters for Low-Rank Adaptation, targeting specific layers (q_proj, v_proj, etc.) to enable fine-tuning only on a subset of the model parameters.\n",
        "\n",
        "10. Training Configuration (SFTConfig)\n",
        "\n",
        "    Specifies all training parameters (e.g., batch sizes, learning rate, logging steps, save steps, evaluation strategy).\n",
        "    Configures integration with W&B, Hugging Face Hub, and sequence length limits.\n",
        "\n",
        "11. Fine-Tuning (SFTTrainer)\n",
        "\n",
        "    Uses the SFTTrainer class to apply LoRA fine-tuning to the LLaMA model, optimizing only selected parameters for efficiency.\n",
        "\n",
        "12. Model Push to Hugging Face Hub\n",
        "\n",
        "    Saves and uploads the fine-tuned model to a private repository on the Hugging Face Hub for future use.\n",
        "\n",
        "13. Experiment Finalization\n",
        "\n",
        "    Closes the Weights & Biases logging session to complete the experiment tracking"
      ],
      "metadata": {
        "id": "eWCtsSTJTn97"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP6BghbIQRHRFVrc5BFZkxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}