{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Fine-Tuning the Sentence Transformer Model\n",
    "\n",
    "**Objective:** To improve the performance of our recommendation system by making the embedding model an \"expert\" on our specific travel review data.\n",
    "\n",
    "### The \"Why\": Pre-trained vs. Fine-tuned\n",
    "\n",
    "We are using a **pre-trained** model (`all-MiniLM-L6-v2`). It's been trained on a massive, general-purpose dataset from the internet. It's good at understanding general language, but it doesn't know the specific nuances of travel reviews. For example, words like \"vibe,\" \"insta-worthy,\" or \"chill\" might have very specific meanings in a travel context.\n",
    "\n",
    "**Fine-tuning** is the process of taking this pre-trained model and training it a little more on our own specific dataset. This adapts the model to our domain, teaching it the specific vocabulary and semantic relationships in our data. The goal is that after fine-tuning, reviews for the *same place* will have more similar embeddings than they did before.\n",
    "\n",
    "### The Approach: Contrastive Learning\n",
    "\n",
    "We will use a form of **contrastive learning**. We need to provide the model with examples of what it should consider \"similar\" and \"dissimilar.\"\n",
    "\n",
    "1.  **Positive Pairs (Similar):** Two different reviews for the *same* place. The model should learn to make their embeddings very close.\n",
    "2.  **Negative Pairs (Dissimilar):** Two reviews for *different* places. The model should learn to make their embeddings far apart.\n",
    "\n",
    "We will use a specific loss function, `MultipleNegativesRankingLoss`, which is highly efficient and effective for this type of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 73722 reviews.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import InputExample\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "# --- Load the pre-processed data from a previous notebook ---\n",
    "# This is the dataframe that has one row per review\n",
    "PATH = \"/Users/amanjaiswal/Work/hop_v3/backend/combined_results.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "import json\n",
    "def safe_literal_eval(s):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "df['detailed_reviews'] = df['detailed_reviews'].apply(safe_literal_eval)\n",
    "df_reviews_exploded = df[['place_id', 'name', 'detailed_reviews']].explode('detailed_reviews')\n",
    "df_reviews_exploded_filtered = df_reviews_exploded[df_reviews_exploded['detailed_reviews'].apply(lambda x: isinstance(x, dict) and bool(x))]\n",
    "df_reviews = pd.json_normalize(df_reviews_exploded_filtered['detailed_reviews'])\n",
    "df_reviews['place_id'] = df_reviews_exploded_filtered['place_id'].values\n",
    "df_reviews['place_name'] = df_reviews_exploded_filtered['name'].values\n",
    "\n",
    "# Clean up the data: we only need reviews with actual text\n",
    "flat_df = df_reviews[['place_id', 'place_name', 'review_text']].copy()\n",
    "flat_df.dropna(subset=['review_text'], inplace=True)\n",
    "flat_df = flat_df[flat_df['review_text'].str.strip() != '']\n",
    "\n",
    "print(f\"Loaded {len(flat_df)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4894de48c48747bd8c9cd5ede9f7f3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training pairs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10000 training examples.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Training Examples ---\n",
    "\n",
    "# Group reviews by place\n",
    "reviews_by_place = flat_df.groupby('place_id')['review_text'].apply(list)\n",
    "\n",
    "# We only want places with at least 2 reviews to form pairs\n",
    "places_with_multiple_reviews = reviews_by_place[reviews_by_place.apply(len) >= 2]\n",
    "\n",
    "train_examples = []\n",
    "for place_id, reviews in tqdm(places_with_multiple_reviews.items(), desc=\"Creating training pairs\"):\n",
    "    # For each place, we treat all its reviews as a positive group.\n",
    "    # The MultipleNegativesRankingLoss will automatically create positive and negative pairs.\n",
    "    # For example, for a place with reviews [r1, r2, r3], it will create positive pairs (r1, r2), (r1, r3), (r2, r3)\n",
    "    # and contrast them with reviews from other places in the same batch.\n",
    "    for i in range(len(reviews) - 1):\n",
    "        train_examples.append(InputExample(texts=[reviews[i], reviews[i+1]]))\n",
    "\n",
    "# For demonstration, we'll just use a sample of the data to keep training fast\n",
    "random.shuffle(train_examples)\n",
    "train_sample = train_examples[:10000] # Use up to 10,000 examples for training\n",
    "\n",
    "print(f\"Created {len(train_sample)} training examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# --- Load the pre-trained model ---\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name, device='cpu')\n",
    "\n",
    "# --- Define the loss function ---\n",
    "# This loss is ideal for our task. It takes a batch of sentences and assumes that\n",
    "# sentences from the same InputExample are positive pairs, and all others are negative.\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# --- Create a DataLoader ---\n",
    "# The DataLoader will batch our training examples.\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_sample, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# --- Start the training process ---\n",
    "num_epochs = 1\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) # 10% of training steps for warmup\n",
    "\n",
    "print(\"Starting the fine-tuning process...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='./fine_tuned_model',\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Saving and Using the Fine-Tuned Model\n",
    "\n",
    "The `model.fit()` function has already saved our fine-tuned model to the `./fine_tuned_model` directory. This directory now contains everything needed to load the new, specialized model.\n",
    "\n",
    "You can now use this model in your API by simply changing the model name in your `main.py` from `'sentence-transformers/all-MiniLM-L6-v2'` to `'./fine_tuned_model'`. This will load your specialized model instead of the general-purpose one, and all your recommendations should now be based on its improved understanding of travel reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded successfully.\n",
      "Similarity between two similar reviews: 0.7595\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "fine_tuned_model = SentenceTransformer('./fine_tuned_model')\n",
    "\n",
    "\n",
    "# You can now use this `fine_tuned_model` object to encode sentences, and it will have a better\n",
    "# understanding of your specific data.\n",
    "print(\"Fine-tuned model loaded successfully.\")\n",
    "\n",
    "# For example, let's compare its embeddings for two reviews of the same place\n",
    "review1 = \"This place had such a great vibe, very chill and relaxing.\"\n",
    "review2 = \"Loved the atmosphere here, it was a perfect spot to unwind.\"\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedding1 = fine_tuned_model.encode(review1)\n",
    "embedding2 = fine_tuned_model.encode(review2)\n",
    "\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "print(f\"Similarity between two similar reviews: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload trained model zip to gdrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "destination_path = \"/content/drive/MyDrive/fine_tuned_model.zip\"\n",
    "shutil.move('fine_tuned_model.zip', destination_path)\n",
    "print(f\"Zipped model successfully uploaded to Google Drive at: {destination_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
